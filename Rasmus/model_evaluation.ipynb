{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import module.dataloader as dataloader\n",
    "from module.utils import from_xywh_to_min_max\n",
    "from tqdm import tqdm\n",
    "from model_architecture import Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPOSAL_SIZE = (128, 128)\n",
    "BATCH_SIZE = 1200\n",
    "BALANCE = 0.5\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "normalize_only = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert NumPy array to PIL Image\n",
    "    transforms.ToTensor(),    # Convert PIL Image to Tensor [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize the tensor\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_test = dataloader.PotholeDataset(\n",
    "    '../Potholes/annotated-images/',\n",
    "    '../Potholes/labeled_proposals/',\n",
    "    '../Potholes/annotated-images/',\n",
    "    transform=normalize_only, \n",
    "    proposals_per_batch=BATCH_SIZE,\n",
    "    proposal_size=PROPOSAL_SIZE,\n",
    "    balance=BALANCE,\n",
    "    split='train'\n",
    ")\n",
    "# dataset_test = dataloader.PotholeDataset('../Potholes/annotated-images/', '../Potholes/labeled_proposals/', '../Potholes/annotated-images/', proposals_per_batch=BATCH_SIZE, proposal_size=PROPOSAL_SIZE, balance=BALANCE, split='test')\n",
    "\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=4)\n",
    "# val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=1,num_workers=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2601450/1737561638.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('../models/model_2.pth'))\n"
     ]
    }
   ],
   "source": [
    "model = Network(PROPOSAL_SIZE)\n",
    "model.load_state_dict(torch.load('../models/model_2.pth'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Validate the model using a validation dataloader and calculate accuracy and average precision (AP).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
    "        device (torch.device): The device to run validation on.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing validation accuracy and AP.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for single_val_dict in tqdm(val_loader, total=len(val_loader)):\n",
    "            # Load data and move to device\n",
    "            proposal_image_val = single_val_dict['proposal_images'][0].to(device)\n",
    "            label_val = single_val_dict['labels'][0].to(device)\n",
    "            proposals = single_val_dict[\"proposals\"][0].to(device).float()  # Ensure proposals are float\n",
    "\n",
    "            # Ensure labels have the correct shape\n",
    "            label_val = label_val.squeeze(-1).float()\n",
    "\n",
    "            # Get model predictions and apply sigmoid\n",
    "            output = model(proposal_image_val)\n",
    "            output = torch.sigmoid(output).squeeze(-1)  # Shape: [N]\n",
    "            # Perform NMS to filter proposals and scores\n",
    "            nms_indices = torchvision.ops.nms(proposals, output, 0.5)\n",
    "\n",
    "            # Filter proposals, scores, and labels using NMS indices\n",
    "            filtered_output = output[nms_indices]\n",
    "            filtered_labels = label_val[nms_indices]\n",
    "\n",
    "            # Collect outputs and labels for AP calculation\n",
    "            all_outputs.append(filtered_output.cpu().numpy())\n",
    "            all_labels.append(filtered_labels.cpu().numpy())\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = filtered_output > 0.5\n",
    "            val_correct += (filtered_labels == predicted).sum().cpu().item()\n",
    "\n",
    "    # Flatten all outputs and labels\n",
    "    all_outputs = torch.cat([torch.tensor(x) for x in all_outputs]).numpy()\n",
    "    all_labels = torch.cat([torch.tensor(x) for x in all_labels]).numpy()\n",
    "\n",
    "    # Calculate Average Precision (AP) using sklearn\n",
    "    ap_score = average_precision_score(all_labels, all_outputs)\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = val_correct / len(all_labels)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Validation Accuracy: {val_accuracy * 100:.1f}%\")\n",
    "    print(f\"Average Precision (AP): {ap_score:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'val_acc': val_accuracy,\n",
    "        'ap': ap_score\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def validate_per_image(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Validate the model per image using a validation dataloader and calculate accuracy and average precision (AP).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
    "        device (torch.device): The device to run validation on.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing average validation accuracy and AP per image.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    total_ap = 0\n",
    "    image_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for single_val_dict in tqdm(val_loader, total=len(val_loader)):\n",
    "            # Load data and move to device (processing one image at a time)\n",
    "            proposal_image_val = single_val_dict['proposal_images'][0].to(device)\n",
    "            label_val = single_val_dict['labels'][0].to(device)\n",
    "            proposals = single_val_dict[\"proposals\"][0]  # Ensure proposals are float\n",
    "            proposals = [from_xywh_to_min_max(p) for p in proposals.numpy()]\n",
    "            proposals = torch.tensor(proposals).to(device).float()\n",
    "            # Ensure labels have the correct shape\n",
    "            label_val = label_val.squeeze(-1).float()\n",
    "\n",
    "            # Get model predictions and apply sigmoid\n",
    "            output = model(proposal_image_val)\n",
    "            output = torch.sigmoid(output).squeeze(-1)  # Shape: [N]\n",
    "            # Perform NMS to filter proposals and scores\n",
    "            nms_indices = torchvision.ops.nms(proposals, output, 0.5)\n",
    "\n",
    "            # Filter proposals, scores, and labels using NMS indices\n",
    "            filtered_output = output[nms_indices]\n",
    "            filtered_labels = label_val[nms_indices]\n",
    "\n",
    "            # Calculate AP and accuracy for the single image\n",
    "            if len(filtered_labels) > 0:\n",
    "                # AP calculation\n",
    "                ap_score = average_precision_score(\n",
    "                    filtered_labels.cpu().numpy(), filtered_output.cpu().numpy()\n",
    "                )\n",
    "                total_ap += ap_score\n",
    "\n",
    "                # Accuracy calculation\n",
    "                predicted = filtered_output > 0.5\n",
    "                accuracy = (filtered_labels == predicted).sum().cpu().item() / len(filtered_labels)\n",
    "                total_accuracy += accuracy\n",
    "\n",
    "            image_count += 1\n",
    "\n",
    "    # Average metrics across all images\n",
    "    avg_accuracy = total_accuracy / image_count if image_count > 0 else 0\n",
    "    avg_ap = total_ap / image_count if image_count > 0 else 0\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Average Validation Accuracy: {avg_accuracy * 100:.1f}%\")\n",
    "    print(f\"Average Precision (AP): {avg_ap:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'val_acc': avg_accuracy,\n",
    "        'ap': avg_ap\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/463 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/463 [00:03<09:56,  1.29s/it]/zhome/81/e/154648/repos/deep-cv-test/test-environment/lib64/python3.9/site-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 88/463 [00:38<02:48,  2.23it/s]/zhome/81/e/154648/repos/deep-cv-test/test-environment/lib64/python3.9/site-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 127/463 [00:55<02:49,  1.98it/s]/zhome/81/e/154648/repos/deep-cv-test/test-environment/lib64/python3.9/site-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 209/463 [01:29<01:29,  2.82it/s]/zhome/81/e/154648/repos/deep-cv-test/test-environment/lib64/python3.9/site-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "100%|██████████| 463/463 [03:04<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Accuracy: 86.9%\n",
      "Average Precision (AP): 0.4078\n",
      "{'val_acc': 0.8686414483346563, 'ap': 0.4077591232973656}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(validate_per_image(model, test_loader, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "def visualize_image(image, boxes,labels, proposals=None):\n",
    "    # Adjust ground truth boxes according to the scale\n",
    "    \n",
    "    # Convert color for display\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    # Draw Selective Search proposals in green if provided\n",
    "    if proposals is not None:\n",
    "        for (x, y, w, h), label in zip(proposals,labels):\n",
    "            # Adjust Selective Search boxes according to the scale\n",
    "\n",
    "\n",
    "            x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "            if label == 1:\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 1)\n",
    "                \n",
    "            # cv2.putText(image, (15, 15), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Draw ground truth boxes in blue\n",
    "    for (xmin, ymin, xmax, ymax) in boxes:\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_nms_boxes(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Validate the model per image using a validation dataloader and calculate accuracy and average precision (AP).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
    "        device (torch.device): The device to run validation on.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing average validation accuracy and AP per image.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    total_ap = 0\n",
    "    image_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        iterator = iter(val_loader)\n",
    "        img = next(iterator)\n",
    "        # Load data and move to device (processing one image at a time)\n",
    "        proposal_image_val = img['proposal_images'][0].to(device)\n",
    "        proposal_image = img[\"image\"][0]\n",
    "        label_val = img['labels'][0].to(device)\n",
    "        proposals = img[\"proposals\"][0]  # Ensure proposals are float\n",
    "        proposals = [from_xywh_to_min_max(p) for p in proposals.numpy()]\n",
    "        proposals = torch.tensor(proposals).to(device).float()\n",
    "        # Ensure labels have the correct shape\n",
    "        label_val = label_val.squeeze(-1).float()\n",
    "\n",
    "        # Get model predictions and apply sigmoid\n",
    "        output = model(proposal_image_val)\n",
    "        output = torch.sigmoid(output).squeeze(-1)  # Shape: [N]\n",
    "        # Perform NMS to filter proposals and scores\n",
    "        nms_indices = torchvision.ops.nms(proposals, output, 0.5)\n",
    "\n",
    "        # Filter proposals, scores, and labels using NMS indices\n",
    "        filtered_output = output[nms_indices]\n",
    "        filtered_labels = label_val[nms_indices]\n",
    "        filtered_proposals = proposals[nms_indices]\n",
    "        \n",
    "        visualize_image()\n",
    "        \n",
    "        \n",
    "\n",
    "    # Average metrics across all images\n",
    "    avg_accuracy = total_accuracy / image_count if image_count > 0 else 0\n",
    "    avg_ap = total_ap / image_count if image_count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
