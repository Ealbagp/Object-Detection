{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import module.dataloader as dataloader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model_architecture import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPOSAL_SIZE = (128, 128)\n",
    "BATCH_SIZE = 50\n",
    "BALANCE = 0.5\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert NumPy array to PIL Image\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),    # Convert PIL Image to Tensor [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize the tensor\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "normalize_only = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert NumPy array to PIL Image\n",
    "    transforms.ToTensor(),    # Convert PIL Image to Tensor [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize the tensor\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "dataset_train = dataloader.PotholeDataset(\n",
    "    '../Potholes/annotated-images/',\n",
    "    '../Potholes/labeled_proposals/',\n",
    "    '../Potholes/annotated-images/',\n",
    "    transform=transform,\n",
    "    proposals_per_batch=BATCH_SIZE,\n",
    "    proposal_size=PROPOSAL_SIZE,\n",
    "    balance=BALANCE,\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "dataset_val = dataloader.PotholeDataset(\n",
    "    '../Potholes/annotated-images/',\n",
    "    '../Potholes/labeled_proposals/',\n",
    "    '../Potholes/annotated-images/',\n",
    "    transform=normalize_only, \n",
    "    proposals_per_batch=BATCH_SIZE,\n",
    "    proposal_size=PROPOSAL_SIZE,\n",
    "    balance=BALANCE,\n",
    "    split='val'\n",
    ")\n",
    "# dataset_test = dataloader.PotholeDataset('../Potholes/annotated-images/', '../Potholes/labeled_proposals/', '../Potholes/annotated-images/', proposals_per_batch=BATCH_SIZE, proposal_size=PROPOSAL_SIZE, balance=BALANCE, split='test')\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "model = Network(proposal_size=PROPOSAL_SIZE)\n",
    "model.apply(init_weights)\n",
    "model.to(device)\n",
    "#Initialize the optimizer\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epochs=10):\n",
    "    def loss_fun(output, target):\n",
    "        return F.cross_entropy(output, target, reduction='mean', weight=torch.tensor([1.0, 2.0]).to(device))\n",
    "    \n",
    "    out_dict = {\n",
    "              'train_acc': [],\n",
    "              'val_acc': [],\n",
    "              'train_loss': [],\n",
    "              'val_loss': []}\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "        # for minibatch_no, (data, target) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        for idx, (single_image_dict) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            # for proposal, label, proposal_image in zip(single_image_dict['proposals'], single_image_dict['labels'], single_image_dict['proposal_images']):\n",
    "            proposal_image, label = single_image_dict['proposal_images'][0].to(device), single_image_dict['labels'][0].to(device)\n",
    "            #Zero the gradients computed for each weight\n",
    "            optimizer.zero_grad()\n",
    "            #Forward pass your image through the network\n",
    "            output = model(proposal_image)\n",
    "            #Compute the loss\n",
    "            loss = loss_fun(output, label)\n",
    "            #Backward pass through the network\n",
    "            loss.backward()\n",
    "            #Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            #Compute how many were correctly classified\n",
    "            output = nn.functional.softmax(output)\n",
    "            predicted = output.argmax(1)\n",
    "            train_correct += (label==predicted).sum().cpu().item()\n",
    "\n",
    "        #Comput the test accuracy\n",
    "        val_loss = []\n",
    "        val_correct = 0\n",
    "        model.eval()\n",
    "        for single_val_dict in val_loader:\n",
    "            # for proposal_val, label_val, proposal_image_val in zip(single_val_dict['proposals'], single_val_dict['labels'], single_val_dict['proposal_images']):\n",
    "            proposal_image_val, label_val = single_val_dict['proposal_images'][0].to(device), single_val_dict['labels'][0].to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(proposal_image_val)\n",
    "\n",
    "            val_loss.append(loss_fun(output, label_val).cpu().item())\n",
    "            output = nn.functional.softmax(output)\n",
    "            predicted = output.argmax(1)\n",
    "            val_correct += (label_val==predicted).sum().cpu().item()\n",
    "\n",
    "        out_dict['train_acc'].append(train_correct/len(dataset_train)/BATCH_SIZE)\n",
    "        out_dict['val_acc'].append(val_correct/len(dataset_val)/BATCH_SIZE)\n",
    "        out_dict['train_loss'].append(np.mean(train_loss))\n",
    "        out_dict['val_loss'].append(np.mean(val_loss))\n",
    "\n",
    "        print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(val_loss):.3f}\\t\",\n",
    "              f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['val_acc'][-1]*100:.1f}%\") # Dividing by 5 because of the batch_size\n",
    "        \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?epoch/s]/zhome/81/e/154648/repos/Object-Detection/Rasmus/../module/dataloader.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels[selected_indices], dtype=torch.long) # Class indices should be long for torch to work.\n",
      "/zhome/81/e/154648/repos/Object-Detection/Rasmus/../module/dataloader.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels[selected_indices], dtype=torch.long) # Class indices should be long for torch to work.\n",
      "\n",
      "/zhome/81/e/154648/repos/Object-Detection/Rasmus/../module/dataloader.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels[selected_indices], dtype=torch.long) # Class indices should be long for torch to work./zhome/81/e/154648/repos/Object-Detection/Rasmus/../module/dataloader.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels[selected_indices], dtype=torch.long) # Class indices should be long for torch to work.\n",
      "/tmp/ipykernel_2078153/2883346317.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = nn.functional.softmax(output)\n",
      "100%|██████████| 463/463 [00:16<00:00, 28.93it/s]\n",
      "/zhome/81/e/154648/repos/Object-Detection/Rasmus/../module/dataloader.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels[selected_indices], dtype=torch.long) # Class indices should be long for torch to work.\n",
      "/tmp/ipykernel_2078153/2883346317.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = nn.functional.softmax(output)\n",
      " 10%|█         | 1/10 [00:20<03:07, 20.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.450\t test: 1.139\t Accuracy train: 71.9%\t test: 57.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:16<00:00, 28.69it/s]\n",
      " 20%|██        | 2/10 [00:41<02:47, 20.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.431\t test: 1.299\t Accuracy train: 72.8%\t test: 55.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:15<00:00, 28.99it/s]\n",
      " 30%|███       | 3/10 [01:02<02:25, 20.84s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.439\t test: 1.434\t Accuracy train: 72.6%\t test: 57.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:16<00:00, 28.90it/s]\n",
      " 40%|████      | 4/10 [01:23<02:04, 20.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.435\t test: 1.276\t Accuracy train: 72.4%\t test: 55.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:15<00:00, 29.08it/s]\n",
      " 50%|█████     | 5/10 [01:44<01:43, 20.77s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.425\t test: 1.003\t Accuracy train: 73.2%\t test: 58.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:16<00:00, 28.86it/s]\n",
      " 60%|██████    | 6/10 [02:04<01:23, 20.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.420\t test: 1.641\t Accuracy train: 73.6%\t test: 54.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:15<00:00, 29.02it/s]\n",
      " 70%|███████   | 7/10 [02:25<01:02, 20.76s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.421\t test: 1.380\t Accuracy train: 73.7%\t test: 55.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:16<00:00, 28.76it/s]\n",
      " 80%|████████  | 8/10 [02:46<00:41, 20.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.415\t test: 1.873\t Accuracy train: 74.0%\t test: 54.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:16<00:00, 28.78it/s]\n",
      " 90%|█████████ | 9/10 [03:07<00:20, 20.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.409\t test: 1.213\t Accuracy train: 73.9%\t test: 56.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:16<00:00, 28.80it/s]\n",
      "100%|██████████| 10/10 [03:28<00:00, 20.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.424\t test: 1.563\t Accuracy train: 73.7%\t test: 56.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_acc': [0.7189200863930886,\n",
       "  0.7281209503239741,\n",
       "  0.7263930885529157,\n",
       "  0.7241900647948164,\n",
       "  0.7315334773218143,\n",
       "  0.7359827213822894,\n",
       "  0.7367602591792657,\n",
       "  0.7400431965442764,\n",
       "  0.7390064794816414,\n",
       "  0.7368898488120951],\n",
       " 'val_acc': [0.5761616161616162,\n",
       "  0.5507070707070707,\n",
       "  0.5775757575757576,\n",
       "  0.554949494949495,\n",
       "  0.5860606060606061,\n",
       "  0.5480808080808081,\n",
       "  0.5577777777777778,\n",
       "  0.5468686868686868,\n",
       "  0.5612121212121213,\n",
       "  0.5632323232323233],\n",
       " 'train_loss': [0.4495132342141854,\n",
       "  0.43061876235847596,\n",
       "  0.43865889613365766,\n",
       "  0.434578953934411,\n",
       "  0.4252537651305322,\n",
       "  0.42024923193210145,\n",
       "  0.4210601620874961,\n",
       "  0.4151010038270806,\n",
       "  0.4093325950897281,\n",
       "  0.4244507958159076],\n",
       " 'val_loss': [1.1385483312787432,\n",
       "  1.2992016257661763,\n",
       "  1.433503128287166,\n",
       "  1.275570178423265,\n",
       "  1.0034209306191917,\n",
       "  1.6413017074869136,\n",
       "  1.3798924570131783,\n",
       "  1.8726759173653342,\n",
       "  1.2132604037872468,\n",
       "  1.56289015469527]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'pothole_detection_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
