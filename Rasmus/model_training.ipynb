{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import module.dataloader as dataloader\n",
    "from tqdm import tqdm\n",
    "from model_architecture import Network\n",
    "from model_architecture_improved import NetworkImproved, CustomCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPOSAL_SIZE = (128, 128)\n",
    "BATCH_SIZE = 100\n",
    "BALANCE = 0.5\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert NumPy array to PIL Image\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),    # Convert PIL Image to Tensor [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize the tensor\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "normalize_only = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert NumPy array to PIL Image\n",
    "    transforms.ToTensor(),    # Convert PIL Image to Tensor [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize the tensor\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "dataset_train = dataloader.PotholeDataset(\n",
    "    '../Potholes/annotated-images/',\n",
    "    '../Potholes/labeled_proposals/',\n",
    "    '../Potholes/annotated-images/',\n",
    "    transform=transform,\n",
    "    proposals_per_batch=BATCH_SIZE,\n",
    "    proposal_size=PROPOSAL_SIZE,\n",
    "    balance=BALANCE,\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "dataset_val = dataloader.PotholeDataset(\n",
    "    '../Potholes/annotated-images/',\n",
    "    '../Potholes/labeled_proposals/',\n",
    "    '../Potholes/annotated-images/',\n",
    "    transform=normalize_only, \n",
    "    proposals_per_batch=BATCH_SIZE,\n",
    "    proposal_size=PROPOSAL_SIZE,\n",
    "    balance=BALANCE,\n",
    "    split='val'\n",
    ")\n",
    "# dataset_test = dataloader.PotholeDataset('../Potholes/annotated-images/', '../Potholes/labeled_proposals/', '../Potholes/annotated-images/', proposals_per_batch=BATCH_SIZE, proposal_size=PROPOSAL_SIZE, balance=BALANCE, split='test')\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "model = Network(proposal_size=PROPOSAL_SIZE)\n",
    "model.apply(initialize_weights)\n",
    "model.to(device)\n",
    "#Initialize the optimizer\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train(model, optimizer, num_epochs=10):\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "    \n",
    "    \n",
    "    def loss_fun(output, target):\n",
    "        pos_weight = torch.tensor([4.0]).to(device)\n",
    "        return F.binary_cross_entropy_with_logits(output, target, pos_weight=pos_weight)\n",
    "    \n",
    "    out_dict = {\n",
    "              'train_acc': [],\n",
    "              'val_acc': [],\n",
    "              'train_loss': [],\n",
    "              'val_loss': []}\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "        # for minibatch_no, (data, target) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        for idx, (single_image_dict) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            # for proposal, label, proposal_image in zip(single_image_dict['proposals'], single_image_dict['labels'], single_image_dict['proposal_images']):\n",
    "            proposal_image, label = single_image_dict['proposal_images'][0].to(device), single_image_dict['labels'][0].to(device)\n",
    "            label = label.unsqueeze(1).float()\n",
    "            #Zero the gradients computed for each weight\n",
    "            optimizer.zero_grad()\n",
    "            #Forward pass your image through the network\n",
    "            output = model(proposal_image)\n",
    "            #Compute the loss\n",
    "            loss = loss_fun(output, label)\n",
    "            #Backward pass through the network\n",
    "            loss.backward()\n",
    "            #Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            #Compute how many were correctly classified\n",
    "            output = nn.functional.sigmoid(output)\n",
    "            predicted = output > 0.5\n",
    "            train_correct += (label==predicted).sum().cpu().item() / len(label)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        #Comput the test accuracy\n",
    "        val_loss = []\n",
    "        val_correct = 0\n",
    "        model.eval()\n",
    "        for single_val_dict in val_loader:\n",
    "            # for proposal_val, label_val, proposal_image_val in zip(single_val_dict['proposals'], single_val_dict['labels'], single_val_dict['proposal_images']):\n",
    "            proposal_image_val, label_val = single_val_dict['proposal_images'][0].to(device), single_val_dict['labels'][0].to(device)\n",
    "            label_val = label_val.unsqueeze(1).float()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(proposal_image_val)\n",
    "\n",
    "            val_loss.append(loss_fun(output, label_val).cpu().item())\n",
    "            output = nn.functional.sigmoid(output)\n",
    "            predicted = output > 0.5\n",
    "            val_correct += (label_val==predicted).sum().cpu().item() / len(label_val)\n",
    "\n",
    "        out_dict['train_acc'].append(train_correct/len(dataset_train))\n",
    "        out_dict['val_acc'].append(val_correct/len(dataset_val))\n",
    "        out_dict['train_loss'].append(np.mean(train_loss))\n",
    "        out_dict['val_loss'].append(np.mean(val_loss))\n",
    "\n",
    "        print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(val_loss):.3f}\\t\",\n",
    "              f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['val_acc'][-1]*100:.1f}%\") # Dividing by 5 because of the batch_size\n",
    "        \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:35<00:00, 13.09it/s]\n",
      " 10%|█         | 1/10 [00:48<07:15, 48.34s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.745\t test: 1.284\t Accuracy train: 76.5%\t test: 73.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:36<00:00, 12.77it/s]\n",
      " 20%|██        | 2/10 [01:37<06:29, 48.73s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.660\t test: 1.256\t Accuracy train: 78.8%\t test: 76.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:36<00:00, 12.83it/s]\n",
      " 30%|███       | 3/10 [02:26<05:41, 48.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.635\t test: 1.258\t Accuracy train: 79.4%\t test: 75.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:36<00:00, 12.80it/s]\n",
      " 40%|████      | 4/10 [03:15<04:53, 48.89s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.635\t test: 1.378\t Accuracy train: 79.6%\t test: 76.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:36<00:00, 12.83it/s]\n",
      " 50%|█████     | 5/10 [04:04<04:04, 48.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.627\t test: 1.222\t Accuracy train: 79.5%\t test: 76.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:36<00:00, 12.72it/s]\n",
      " 60%|██████    | 6/10 [04:53<03:16, 49.08s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.629\t test: 1.261\t Accuracy train: 79.2%\t test: 75.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:35<00:00, 13.10it/s]\n",
      " 70%|███████   | 7/10 [05:41<02:26, 48.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.632\t test: 1.219\t Accuracy train: 79.4%\t test: 75.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:35<00:00, 13.07it/s]\n",
      " 80%|████████  | 8/10 [06:29<01:37, 48.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.630\t test: 1.001\t Accuracy train: 79.6%\t test: 75.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:34<00:00, 13.43it/s]\n",
      " 90%|█████████ | 9/10 [07:17<00:48, 48.29s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.626\t test: 1.076\t Accuracy train: 79.6%\t test: 76.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [00:36<00:00, 12.72it/s]\n",
      "100%|██████████| 10/10 [08:07<00:00, 48.72s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.632\t test: 1.085\t Accuracy train: 79.3%\t test: 76.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_acc': [0.764614431945842,\n",
       "  0.788123208879375,\n",
       "  0.7942306250526412,\n",
       "  0.7960226581720582,\n",
       "  0.794760249939252,\n",
       "  0.791658771587995,\n",
       "  0.7942055819650233,\n",
       "  0.7960749125770411,\n",
       "  0.7964621019673941,\n",
       "  0.7928781214550903],\n",
       " 'val_acc': [0.73681853878471,\n",
       "  0.7598549255973275,\n",
       "  0.7586481425699927,\n",
       "  0.7610626320596421,\n",
       "  0.7632519994444198,\n",
       "  0.7563805245412273,\n",
       "  0.7551878956841381,\n",
       "  0.7536868752565169,\n",
       "  0.7636374655830314,\n",
       "  0.7661844605639295],\n",
       " 'train_loss': [0.744725150500724,\n",
       "  0.6601844908866717,\n",
       "  0.6349266919731836,\n",
       "  0.6352231899682698,\n",
       "  0.627282217692093,\n",
       "  0.6292411004621575,\n",
       "  0.631784806241227,\n",
       "  0.6299278192527866,\n",
       "  0.6256811970253485,\n",
       "  0.6318037473228528],\n",
       " 'val_loss': [1.283623400962714,\n",
       "  1.2564314004748758,\n",
       "  1.2584266275769533,\n",
       "  1.3777752044526013,\n",
       "  1.2215861396657095,\n",
       "  1.2610091237407741,\n",
       "  1.2190603114137746,\n",
       "  1.0012043223838614,\n",
       "  1.0757801502642006,\n",
       "  1.0847427530421152]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
