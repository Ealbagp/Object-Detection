{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dataloader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model_architecture import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPOSAL_SIZE = (128, 128)\n",
    "BATCH_SIZE = 100\n",
    "BALANCE = 0.5\n",
    "\n",
    "dataset_train = dataloader.PotholeDataset('./Potholes/annotated-images/', './Potholes/proposals/', './Potholes/annotated-images/', proposals_per_batch=BATCH_SIZE, proposal_size=PROPOSAL_SIZE, balance=BALANCE, split='train')\n",
    "dataset_val = dataloader.PotholeDataset('./Potholes/annotated-images/', './Potholes/proposals/', './Potholes/annotated-images/', proposals_per_batch=BATCH_SIZE, proposal_size=PROPOSAL_SIZE, balance=BALANCE, split='val')\n",
    "dataset_test = dataloader.PotholeDataset('./Potholes/annotated-images/', './Potholes/proposals/', './Potholes/annotated-images/', proposals_per_batch=BATCH_SIZE, proposal_size=PROPOSAL_SIZE, balance=BALANCE, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "model = Network(proposal_size=PROPOSAL_SIZE)\n",
    "model.apply(init_weights)\n",
    "model.to(device)\n",
    "#Initialize the optimizer\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epochs=10):\n",
    "    def loss_fun(output, target):\n",
    "        return F.cross_entropy(output, target)\n",
    "    \n",
    "    out_dict = {\n",
    "              'train_acc': [],\n",
    "              'val_acc': [],\n",
    "              'train_loss': [],\n",
    "              'val_loss': []}\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "        # for minibatch_no, (data, target) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        for idx, (single_image_dict) in tqdm(enumerate(dataset_train), total=len(dataset_train)):\n",
    "            # for proposal, label, proposal_image in zip(single_image_dict['proposals'], single_image_dict['labels'], single_image_dict['proposal_images']):\n",
    "            proposal_image, label = single_image_dict['proposal_images'].to(device), single_image_dict['labels'].to(device)\n",
    "            #Zero the gradients computed for each weight\n",
    "            optimizer.zero_grad()\n",
    "            #Forward pass your image through the network\n",
    "            output = model(proposal_image)\n",
    "            #Compute the loss\n",
    "\n",
    "            # print(f\"Proposal shape: {single_image_dict['proposals'].shape if single_image_dict['proposals'] is not None else 'None'}\")\n",
    "            # print(f\"Proposal image shape: {proposal_image.shape if proposal_image is not None else 'None'}\")\n",
    "            # print(f\"Label: {label}\")\n",
    "            # print(f\"Label shape: {label.shape if isinstance(label, torch.Tensor) else 'Not a Tensor'}\")\n",
    "\n",
    "            loss = loss_fun(output, label)\n",
    "            #Backward pass through the network\n",
    "            loss.backward()\n",
    "            #Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            #Compute how many were correctly classified\n",
    "            predicted = output.argmax(1)\n",
    "            train_correct += (label==predicted).sum().cpu().item()\n",
    "\n",
    "        #Comput the test accuracy\n",
    "        val_loss = []\n",
    "        val_correct = 0\n",
    "        model.eval()\n",
    "        for single_val_dict in dataset_val:\n",
    "            # for proposal_val, label_val, proposal_image_val in zip(single_val_dict['proposals'], single_val_dict['labels'], single_val_dict['proposal_images']):\n",
    "            proposal_image_val, label_val = single_val_dict['proposal_images'].to(device), single_val_dict['labels'].to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(proposal_image_val)\n",
    "\n",
    "            val_loss.append(loss_fun(output, label_val).cpu().item())\n",
    "            predicted = output.argmax(1)\n",
    "            val_correct += (label_val==predicted).sum().cpu().item()\n",
    "\n",
    "        out_dict['train_acc'].append(train_correct/len(dataset_train)/BATCH_SIZE)\n",
    "        out_dict['val_acc'].append(val_correct/len(dataset_val)/BATCH_SIZE)\n",
    "        out_dict['train_loss'].append(np.mean(train_loss))\n",
    "        out_dict['val_loss'].append(np.mean(val_loss))\n",
    "\n",
    "        print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(val_loss):.3f}\\t\",\n",
    "              f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['val_acc'][-1]*100:.1f}%\") # Dividing by 5 because of the batch_size\n",
    "        \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samer\\Documents\\Intro to deep learning for computer vision\\pothole_detector\\dataloader.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels[selected_indices])\n",
      " 71%|███████   | 327/463 [01:02<00:26,  5.23it/s]\n",
      "  0%|          | 0/10 [01:02<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#Update the weights\u001b[39;00m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 36\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#Compute how many were correctly classified\u001b[39;00m\n\u001b[0;32m     38\u001b[0m predicted \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
